{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchvision import models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(name):\n",
    "    # Check if the dataset directory already exists\n",
    "    if os.path.exists(f\"./data/{name}\"):\n",
    "        print(\"Dataset already exists\")\n",
    "        return\n",
    "    else:\n",
    "        # If the dataset directory doesn't exist, create it\n",
    "        os.mkdir(f\"./data/{name}\")\n",
    "    \n",
    "    # Load the Haar cascade classifier for face detection\n",
    "    face_classifier = cv2.CascadeClassifier(\"./haarcascade_frontalface_default.xml\")\n",
    "    \n",
    "    # Function to crop the face from an image\n",
    "    def face_cropped(img):\n",
    "        gray = img\n",
    "        # Detect faces in the image\n",
    "        faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        # If no faces are detected, return None\n",
    "        if len(faces) == ():\n",
    "            return None\n",
    "        \n",
    "        # Extract the coordinates of the bounding box around the face\n",
    "        cropped_face = None\n",
    "        for (x,y,w,h) in faces:\n",
    "            cropped_face = img[y:y+h, x:x+w]\n",
    "        return cropped_face\n",
    "    \n",
    "    # Open the video capture device (webcam)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if the webcam is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Camera not found\")\n",
    "        return\n",
    "    \n",
    "    img_id = 0\n",
    "\n",
    "    # Capture images until a specified number is reached or Enter key is pressed\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # If a face is detected in the frame, save it to the dataset directory\n",
    "        if face_cropped(frame) is not None:\n",
    "            img_id+=1\n",
    "            # Resize the cropped face to a fixed size\n",
    "            face = cv2.resize(face_cropped(frame), (200,200))\n",
    "            # Save the cropped face as an image file\n",
    "            file_name_path = f\"./data/{name}/{img_id}.jpg\"\n",
    "            cv2.imwrite(file_name_path, face)\n",
    "            # Display the cropped face with an ID\n",
    "            cv2.putText(face, str(img_id), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow(\"Cropped Face\", face)\n",
    "            \n",
    "            # Break the loop if the Enter key is pressed or the specified number of images is captured\n",
    "            if cv2.waitKey(1) == 13 or int(img_id) == 100:\n",
    "                break\n",
    "    \n",
    "    # Release the video capture device and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Collecting Samples Complete!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n"
     ]
    }
   ],
   "source": [
    "generate_dataset('parth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Define transformations to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "    # You can add more transformations here if needed (e.g., normalization)\n",
    "])\n",
    "\n",
    "# Path to the root directory containing image folders\n",
    "data_dir = './data'\n",
    "\n",
    "# Load images from the folders with specified transformations\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.9), int(len(dataset)*0.1)])\n",
    "\n",
    "# Get the class names (folders' names within the data directory)\n",
    "class_names = dataset.classes\n",
    "\n",
    "# Create data loaders to iterate over the datasets in batches\n",
    "batch_size = 1  # Set batch size to 1 for demonstration\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Combine data loaders into a dictionary for easy access\n",
    "data_loader = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": test_loader  # Renamed from 'test_loader' to 'val' for validation set\n",
    "}\n",
    "\n",
    "# Iterate over the training dataset using the data loader\n",
    "for images, labels in data_loader['train']:\n",
    "    # Print the shape of the batch of images and labels\n",
    "    print(images.shape, labels.shape)\n",
    "    break  # Break after printing the first batch for demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kartik', 'parth', 'shrikant']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.mobilenet_v2(weights=\"DEFAULT\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=False)\n",
      "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n",
      "After modifying the last layer:\n",
      "Sequential(\n",
      "  (0): Dropout(p=0.4, inplace=True)\n",
      "  (1): Linear(in_features=1280, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the classifier of the model\n",
    "print(model.classifier)\n",
    "\n",
    "# Getting number of output classes\n",
    "num_out_ftrs = len(class_names)\n",
    "\n",
    "# Freeze all layers for fine-tuning (not doing this takes it very long to train)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the number of inputs in the last layer\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "\n",
    "# Modify the last layer of the classifier\n",
    "model.classifier[0] = nn.Dropout(p=0.4, inplace=True)\n",
    "model.classifier[1] = nn.Linear(num_ftrs, num_out_ftrs)\n",
    "\n",
    "# Print the modified last layer of the classifier\n",
    "print(\"After modifying the last layer:\")\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                dataloaders,\n",
    "                criterion = nn.CrossEntropyLoss(),\n",
    "                prev_checkpoint=None,\n",
    "                learning_rate=0.001,\n",
    "                optimizer = None, \n",
    "                schedular=None, \n",
    "                num_epoch=10,\n",
    "                save_checkpoint=False):\n",
    "\n",
    "    # Check if CUDA is available, and move the model to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define default parameters for optimizer if not provided\n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=model.parameters(),\n",
    "            lr=learning_rate\n",
    "        )\n",
    "\n",
    "    # Get dataset and dataloader sizes\n",
    "    dataset_sizes = {\n",
    "        \"train\": len(dataloaders[\"train\"].dataset),\n",
    "        \"val\": len(dataloaders[\"val\"].dataset)\n",
    "    }\n",
    "    dataloader_sizes ={\n",
    "        \"train\": len(dataloaders[\"train\"]),\n",
    "    }\n",
    "\n",
    "    # Define variables for storing loss and accuracy\n",
    "    train_time_list = []\n",
    "\n",
    "    # Load previous checkpoint if provided\n",
    "    if prev_checkpoint != None:\n",
    "        print(model.load_state_dict(prev_checkpoint[\"model_state\"]))\n",
    "        optimizer.load_state_dict(prev_checkpoint[\"optim_state\"])\n",
    "        epochs_completed = prev_checkpoint[\"epoch\"]\n",
    "        epoch_loss_list = prev_checkpoint[\"epoch_losses\"]\n",
    "        train_time = prev_checkpoint[\"time_taken\"]\n",
    "        print('Loaded checkpoint')\n",
    "    else:\n",
    "        epochs_completed = 0\n",
    "        epoch_loss_list = {\n",
    "            \"train\": [],\n",
    "            \"val\": []\n",
    "        }\n",
    "        train_time = 0\n",
    "    \n",
    "    print(f\"Training Started on {device}\")\n",
    "\n",
    "    # Loop through epochs\n",
    "    for epoch in range(epochs_completed, num_epoch+epochs_completed):\n",
    "\n",
    "        # Each epoch has a training and a validation phase\n",
    "        for phase in [\"train\", 'val']:\n",
    "            time_start = time.time()\n",
    "\n",
    "            # Set model to train mode during training, and eval mode during validation\n",
    "            model.train() if phase == \"train\" else model.eval()\n",
    "\n",
    "            epoch_losses = []\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # Use tqdm for progress bar during training\n",
    "            data_loader = tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch+1}/{num_epoch+epochs_completed}')\n",
    "\n",
    "            # Iterate over batches of data\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(images)\n",
    "                    output_one_hot = torch.argmax(outputs, dim=1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training loop\n",
    "                    if phase == \"train\":\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                running_corrects += torch.sum(output_one_hot == labels.data)\n",
    "\n",
    "            # Update learning rate scheduler if provided and in training phase\n",
    "            if schedular != None and phase == \"train\":\n",
    "                    schedular.step()\n",
    "\n",
    "            # Calculate epoch loss and accuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = 100 * running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            # Print loss and accuracy\n",
    "            data_loader.write(f'loss: {epoch_loss} acc: {epoch_acc}')\n",
    "\n",
    "        epoch_time = time.time() - time_start\n",
    "\n",
    "        if phase == \"train\":\n",
    "            train_time_list.append(epoch_time) \n",
    "\n",
    "        epoch_loss_list[phase].append(epoch_losses)\n",
    "\n",
    "    # Update total training time\n",
    "    train_time += sum(train_time_list)\n",
    "\n",
    "    # Create checkpoint dictionary\n",
    "    checkpoint = {\n",
    "        \"epoch\": num_epoch+epochs_completed,\n",
    "        \"criterion\": criterion,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optimizer.state_dict(),\n",
    "        \"epoch_losses\": epoch_loss_list,\n",
    "        \"time_taken\": train_time\n",
    "    }\n",
    "    \n",
    "    # Save checkpoint if specified\n",
    "    if save_checkpoint == True:\n",
    "        file_name = f\"checkpoint_{time.time()}.pth\"\n",
    "        torch.save(checkpoint, file_name)\n",
    "    \n",
    "    return model, checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/5: 100%|██████████| 270/270 [00:07<00:00, 35.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.130750706019225 acc: 37.407407407407405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 1/5: 100%|██████████| 30/30 [00:00<00:00, 46.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0484357794125876 acc: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/5: 100%|██████████| 270/270 [00:06<00:00, 40.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0324967414140702 acc: 48.148148148148145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 2/5: 100%|██████████| 30/30 [00:00<00:00, 48.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9894797921180725 acc: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/5: 100%|██████████| 270/270 [00:06<00:00, 41.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9546482502310364 acc: 55.18518518518518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 3/5: 100%|██████████| 30/30 [00:00<00:00, 49.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.004696120818456 acc: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/5: 100%|██████████| 270/270 [00:06<00:00, 40.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8818027510687158 acc: 66.29629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 4/5: 100%|██████████| 30/30 [00:00<00:00, 45.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9184776902198791 acc: 53.333333333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/5: 100%|██████████| 270/270 [00:06<00:00, 42.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8120905207263098 acc: 72.96296296296296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 5/5: 100%|██████████| 30/30 [00:00<00:00, 49.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8406905780235926 acc: 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the provided data loader and training parameters for 5 epochs.\n",
    "model, checkpoint = train_model(model, data_loader, num_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gradient computation for all parameters in the model.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/5: 100%|██████████| 270/270 [00:57<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.11844719879391724 acc: 97.03703703703704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 1/5: 100%|██████████| 30/30 [00:00<00:00, 43.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.048870255773363167 acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/5: 100%|██████████| 270/270 [00:56<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0007161589090106578 acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 2/5: 100%|██████████| 30/30 [00:00<00:00, 48.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.06623064047671505 acc: 96.66666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/5: 100%|██████████| 270/270 [00:55<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0003377330281013942 acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 3/5: 100%|██████████| 30/30 [00:00<00:00, 50.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.039476608947734346 acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/5: 100%|██████████| 270/270 [00:55<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00019978626757316913 acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 4/5: 100%|██████████| 30/30 [00:00<00:00, 49.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.027144681539963737 acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/5: 100%|██████████| 270/270 [00:59<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00013217514301855032 acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epoch 5/5: 100%|██████████| 30/30 [00:00<00:00, 41.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.02715660879512143 acc: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model for an additional 5 epochs, using the previously trained model and checkpoint.\n",
    "model, checkpoint = train_model(model, data_loader, num_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Screen Closed\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained face detection model.\n",
    "facedetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Check if CUDA (GPU) is available and set the device accordingly.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Open the video capture device (webcam).\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the resolution of the video capture.\n",
    "cap.set(3, 640)  # Width\n",
    "cap.set(4, 480)  # Height\n",
    "\n",
    "# Define the font for displaying text on the image.\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "\n",
    "# Define the image transformations to be applied.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),        # Convert numpy array to PIL Image\n",
    "    transforms.Resize((200, 200)),  # Resize image to 200x200\n",
    "    transforms.ToTensor(),           # Convert image to PyTorch tensor\n",
    "])\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Capture frame-by-frame from the webcam.\n",
    "        success, imgOrignal = cap.read()\n",
    "        \n",
    "        # Detect faces in the captured frame.\n",
    "        faces = facedetect.detectMultiScale(imgOrignal, 1.3, 5)\n",
    "        \n",
    "        # Loop over each detected face.\n",
    "        for x, y, w, h in faces:\n",
    "            # Crop the detected face.\n",
    "            crop_img = imgOrignal[y:y+h, x:x+h]\n",
    "            \n",
    "            # Apply the defined transformation to the cropped face.\n",
    "            img = transform(crop_img)\n",
    "            \n",
    "            # Add batch dimension to the image tensor.\n",
    "            img = img.unsqueeze(0)\n",
    "            \n",
    "            # Move the image tensor to the appropriate device (GPU or CPU).\n",
    "            img = img.to(device)\n",
    "            \n",
    "            # Perform prediction using the trained model.\n",
    "            prediction = model(img)\n",
    "            \n",
    "            # Get the predicted class index.\n",
    "            classIndex = torch.argmax(prediction, dim=1)\n",
    "            \n",
    "            # Draw rectangle around the detected face.\n",
    "            cv2.rectangle(imgOrignal, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.rectangle(imgOrignal, (x, y-40), (x+w, y), (0, 255, 0), -2)\n",
    "            \n",
    "            # Display the predicted class label on the image.\n",
    "            cv2.putText(imgOrignal, str(class_names[classIndex]), (x, y-10), font, 0.75, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Display the resulting image.\n",
    "        cv2.imshow(\"Result\", imgOrignal)\n",
    "        \n",
    "        # Wait for the 'q' key to be pressed to exit the loop.\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # Release the video capture device and close all OpenCV windows.\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Screen Closed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecc3f01f23be7329d6051ae547971e3af0aca3bdad30426c1726ee29bb0d4d97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
